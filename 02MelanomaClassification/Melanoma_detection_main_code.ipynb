{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE7p_GY1prEc"
      },
      "source": [
        "# **Importing Necessary Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCKZ7BZ6pc1M"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import *\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import shutil\n",
        "from torchvision.utils import draw_bounding_boxes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggcR6VwY3E_n"
      },
      "source": [
        "# **Define 4-layer CNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cV1z0EyB3QKg"
      },
      "outputs": [],
      "source": [
        "class SimpleCNN4(nn.Module):\n",
        "    def __init__(self, ftr_size=512):\n",
        "        super(SimpleCNN4, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.fc1 = nn.Linear(200704, ftr_size)\n",
        "\n",
        "        self.flat = nn.Flatten()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool3(x)\n",
        "\n",
        "        x = self.conv4(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.flat(x)\n",
        "        x = F.relu(self.fc1(x))\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzMlPwU43Kvs"
      },
      "source": [
        "# **Define 8-layer CNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W187DqqA3bmD"
      },
      "outputs": [],
      "source": [
        "class SimpleCNN8(nn.Module):\n",
        "    def __init__(self, ftr_size=512):\n",
        "        super(SimpleCNN8, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=255, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv5 = nn.Conv2d(in_channels=255, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool5 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv6 = nn.Conv2d(in_channels=512, out_channels=720, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.conv7 = nn.Conv2d(in_channels=720, out_channels=1024, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.conv8 = nn.Conv2d(in_channels=1024, out_channels=2000, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.fc1 = nn.Linear(98000, ftr_size)\n",
        "\n",
        "        self.flat = nn.Flatten()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool3(x)\n",
        "\n",
        "        x = self.conv4(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool4(x)\n",
        "\n",
        "        x = self.conv5(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool5(x)\n",
        "\n",
        "        x = self.conv6(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.conv7(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.conv8(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.flat(x) # flatten 3x5x2 matrix -----> 1x30 vector\n",
        "        x = F.relu(self.fc1(x))\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21z_ZHRUp7df"
      },
      "source": [
        "# **Definining Attention Network Class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEcxMvLCqFXN"
      },
      "outputs": [],
      "source": [
        "class AttentionNetwork(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, AttentionNet=None, AttentionFtrExtractor=None, GlobalNet=None, num_classes=2, size=224, ftr_size=512):\n",
        "        super(AttentionNetwork, self).__init__()\n",
        "\n",
        "        self.Attention_net = AttentionNet # network for computing attention maps\n",
        "\n",
        "        self.Attention_ftr_extrcator = AttentionFtrExtractor # (local) feature extractor from attention maps\n",
        "\n",
        "        self.global_net = GlobalNet # global feature extrcator\n",
        "\n",
        "        out_size = num_classes\n",
        "        if num_classes == 2:\n",
        "          out_size = 1\n",
        "\n",
        "        # network for aggregating global and local information\n",
        "        self.aggregate_net = nn.Sequential(nn.Linear(ftr_size*2, ftr_size),\n",
        "                             nn.Sigmoid(),\n",
        "                             nn.Linear(ftr_size, out_size),\n",
        "                             nn.Sigmoid())\n",
        "\n",
        "        self.size = size\n",
        "\n",
        "        # fully-connected layers for atttion maps and aggregate network\n",
        "        self.Attention_FC = nn.Sequential(nn.Linear(ftr_size, 4), nn.Sigmoid())\n",
        "        self.classifier_FC = nn.Linear(ftr_size, num_classes)\n",
        "\n",
        "    # Function to draw the attention map boundaries on the image\n",
        "    def draw_attention_map(self, img, x_min, y_min, x_max, y_max):\n",
        "      images = []\n",
        "      for i in range(img.shape[0]):\n",
        "        img_uint8 = torch.round(255*img[i]).to(torch.uint8)\n",
        "        bbox1 = [x_min[i].item(), y_min[i].item(), x_max[i].item(), y_max[i].item()]\n",
        "        bbox = [bbox1]\n",
        "        bbox = torch.tensor(bbox, dtype=torch.int)\n",
        "        img_with_rect=draw_bounding_boxes(img_uint8, bbox,width=6,colors=[(255,0,0)],fill =False,font_size=20)\n",
        "        img_with_rect = (img_with_rect / 255).to(torch.float32)\n",
        "        images.append(img_with_rect)\n",
        "\n",
        "      return images\n",
        "\n",
        "    # Cropping and zooming into the attention region\n",
        "    def crop_zoom(self, image, tx, ty, tl_x, tl_y):\n",
        "        tx_r = (self.size * tx).int() # real tx (since 0 <= tx <= 1)\n",
        "        ty_r = (self.size * ty).int()\n",
        "\n",
        "        tl_x_r = ((self.size / 2) * tl_x + 1).int() # should be at least 1 pixel\n",
        "        tl_y_r = ((self.size / 2) * tl_y + 1).int()\n",
        "\n",
        "        x_min = (tx_r - tl_x_r).clamp(min=0)\n",
        "        x_max = (tx_r + tl_x_r).clamp(max=self.size)\n",
        "        y_min = (ty_r - tl_y_r).clamp(min=0)\n",
        "        y_max = (ty_r + tl_y_r).clamp(max=self.size)\n",
        "\n",
        "        # draw attention region on image\n",
        "        imgs_with_rect = self.draw_attention_map(image, x_min, y_min, x_max, y_max)\n",
        "\n",
        "        # crop and zoom into the attention region of each image in the batch\n",
        "        for i in range(image.shape[0]):\n",
        "          img = image[i][:, y_min[i]:y_max[i], x_min[i]:x_max[i]]\n",
        "          img = F.interpolate(img[None, :, :, :], (224, 224), mode='bilinear')[0]\n",
        "          image[i] = img\n",
        "\n",
        "        return image, imgs_with_rect\n",
        "\n",
        "    # aggregate two vectors by concatenating them\n",
        "    def aggregate(self, vec1, vec2):\n",
        "        return torch.cat((vec1, vec2), dim=1)\n",
        "\n",
        "    # Function definining how the network processes an input batch of images\n",
        "    def forward(self, image):\n",
        "        # Apply attention, crop, and zoom\n",
        "        vec = self.Attention_net(image)\n",
        "        tx, ty, tl_x, tl_y = self.Attention_FC(vec).transpose(0,1)\n",
        "        x_cropped, img_with_rect = self.crop_zoom(image, tx, ty, tl_x, tl_y)\n",
        "\n",
        "        # Local and Global features\n",
        "        ftr_vec_local = self.Attention_ftr_extrcator(x_cropped)\n",
        "        ftr_vec_global = self.global_net(image)\n",
        "\n",
        "        # Aggregate local and global features and process them through the\n",
        "        # aggregating network to get class probabilities\n",
        "        ftr_vec_final = self.aggregate(ftr_vec_local, ftr_vec_global)\n",
        "        probs = self.aggregate_net(ftr_vec_final)\n",
        "\n",
        "        return probs, img_with_rect\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-I56l2kz4Mml"
      },
      "source": [
        "# **Mounting Google Drive to Read Datasets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3L3_9GWtuy00",
        "outputId": "ec715fb7-03c3-47af-e2c4-b0dd18f31318"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-FGGGaM5qZA"
      },
      "source": [
        "# **Define model-building function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xQXQgr_5xPA"
      },
      "outputs": [],
      "source": [
        "# Function to build the model given the our hyper-parameters\n",
        "def build_model(CNN, weight_sharing, ftr_size, num_classes):\n",
        "  if CNN == '8-layer' and weight_sharing:\n",
        "    CNN_model = SimpleCNN8(ftr_size=ftr_size)\n",
        "    attention_net = CNN_model\n",
        "    attention_ftr_extractor = CNN_model\n",
        "    global_net = CNN_model\n",
        "  elif CNN == '8-layer' and not weight_sharing:\n",
        "    attention_net = SimpleCNN8(ftr_size=ftr_size)\n",
        "    attention_ftr_extractor = SimpleCNN8(ftr_size=ftr_size)\n",
        "    global_net = SimpleCNN8(ftr_size=ftr_size)\n",
        "  elif CNN == \"ResNet18\" and weight_sharing:\n",
        "    CNN_model = models.resnet18(pretrained=True)\n",
        "    fc_in_ftrs = CNN_model.fc.in_features\n",
        "    CNN_fc = nn.Linear(fc_in_ftrs, ftr_size)\n",
        "    CNN_model.fc = CNN_fc\n",
        "\n",
        "    attention_net = CNN_model\n",
        "    attention_ftr_extractor = CNN_model\n",
        "    global_net = CNN_model\n",
        "  elif CNN == \"ResNet18\" and not weight_sharing:\n",
        "    CNN_model1 = models.resnet18(pretrained=True)\n",
        "    fc_in_ftrs = CNN_model1.fc.in_features\n",
        "    CNN_model1.fc = nn.Linear(fc_in_ftrs, ftr_size)\n",
        "\n",
        "    CNN_model2 = models.resnet18(pretrained=True)\n",
        "    CNN_model2.fc = nn.Linear(fc_in_ftrs, ftr_size)\n",
        "\n",
        "    CNN_model3 = models.resnet18(pretrained=True)\n",
        "    CNN_model3.fc = nn.Linear(fc_in_ftrs, ftr_size)\n",
        "\n",
        "    attention_net = CNN_model1\n",
        "    attention_ftr_extractor = CNN_model2\n",
        "    global_net = CNN_model3\n",
        "  elif CNN == \"ResNet50\" and weight_sharing:\n",
        "    CNN_model = models.resnet50(pretrained=True)\n",
        "    fc_in_ftrs = CNN_model.fc.in_features\n",
        "    CNN_fc = nn.Linear(fc_in_ftrs, ftr_size)\n",
        "    CNN_model.fc = CNN_fc\n",
        "\n",
        "    attention_net = CNN_model\n",
        "    attention_ftr_extractor = CNN_model\n",
        "    global_net = CNN_model\n",
        "  elif CNN == \"ResNet50\" and not weight_sharing:\n",
        "    CNN_model1 = models.resnet50(pretrained=True)\n",
        "    fc_in_ftrs = CNN_model1.fc.in_features\n",
        "    CNN_model1.fc = nn.Linear(fc_in_ftrs, ftr_size)\n",
        "\n",
        "    CNN_model2 = models.resnet50(pretrained=True)\n",
        "    CNN_model2.fc = nn.Linear(fc_in_ftrs, ftr_size)\n",
        "\n",
        "    CNN_model3 = models.resnet50(pretrained=True)\n",
        "    CNN_model3.fc = nn.Linear(fc_in_ftrs, ftr_size)\n",
        "\n",
        "    attention_net = CNN_model1\n",
        "    attention_ftr_extractor = CNN_model2\n",
        "    global_net = CNN_model3\n",
        "\n",
        "  # Initialize the model and return it\n",
        "  model = AttentionNetwork(AttentionNet=attention_net,\n",
        "                           AttentionFtrExtractor=attention_ftr_extractor,\n",
        "                           GlobalNet=global_net,\n",
        "                           num_classes=num_classes,\n",
        "                           size=224,\n",
        "                           ftr_size=ftr_size)\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sa_KCoVwwNV6"
      },
      "source": [
        "# **Defining function that logs trial results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkopzQA6wT__"
      },
      "outputs": [],
      "source": [
        "# Log results in the file for which the file descriptor (fd) is given\n",
        "def log_results(CNN, ftr_size, weight_sharing, fd, conf_mat, overall_acc, acc, sensitivity, specificity, percision, loss):\n",
        "  fd.write(\"\\n\")\n",
        "  for i in range(len(conf_mat)):\n",
        "    for j in range(len(conf_mat[i])):\n",
        "      fd.write(str(conf_mat[i][j]))\n",
        "      if j == len(conf_mat[i]) - 1:\n",
        "        fd.write('\\n')\n",
        "      else:\n",
        "        fd.write(' ')\n",
        "\n",
        "  param_string = CNN+' '+str(ftr_size)+' '+str(weight_sharing)+' '\n",
        "  fd.write(param_string+' ')\n",
        "  fd.write(str(overall_acc)+' ')\n",
        "  for i in range(len(acc)):\n",
        "    fd.write(str(acc[i])+' ')\n",
        "  for i in range(len(sensitivity)):\n",
        "    fd.write(str(sensitivity[i])+' ')\n",
        "  for i in range(len(specificity)):\n",
        "    fd.write(str(specificity[i])+' ')\n",
        "  for i in range(len(percision)):\n",
        "    fd.write(str(percision[i])+' ')\n",
        "  fd.write(str(loss))\n",
        "  fd.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PXP3Fox5vO9"
      },
      "source": [
        "# **Define function that computes evaluation metrics**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDHFKX6q502D"
      },
      "outputs": [],
      "source": [
        "# Compute the evaluation metrics, accuracy, sensitivity, specificity, and percision,\n",
        "# for each class\n",
        "def eval_metrics(conf_mat):\n",
        "  acc = [0, 0, 0]\n",
        "  for i in range(3):\n",
        "    acc[i] = round(100*conf_mat[i][i] / (sum(conf_mat[i])), 2)\n",
        "\n",
        "  sensitivity = [0, 0, 0]\n",
        "  for i in range(3):\n",
        "    tp = conf_mat[i][i]\n",
        "    fn = sum(conf_mat[i]) - tp\n",
        "    if tp == 0 and fn == 0:\n",
        "      sensitivity[i] = -1\n",
        "    else:\n",
        "      sensitivity[i] = round(100*tp / (tp + fn), 2)\n",
        "\n",
        "  specificity = [0, 0, 0]\n",
        "  for i in range(3):\n",
        "    tn = sum([conf_mat[j][k] if j != i and k != i else 0 for j in range(3) for k in range(3)])\n",
        "    fp = sum([conf_mat[j][i] if i != j else 0 for j in range(3)])\n",
        "    if tn == 0 and fp == 0:\n",
        "      specificity[i] = -1\n",
        "    else:\n",
        "      specificity[i] = round(100*tn / (tn + fp), 2)\n",
        "\n",
        "  percision = [0, 0, 0]\n",
        "  for i in range(3):\n",
        "    tp = conf_mat[i][i]\n",
        "    fp = sum([conf_mat[j][i] if i != j else 0 for j in range(3)])\n",
        "    if tp == 0 and fp == 0:\n",
        "      percision[i] = -1\n",
        "    else:\n",
        "      percision[i] = round(100*tp / (tp + fp), 2)\n",
        "  return acc, sensitivity, specificity, percision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GjrT4iD068R"
      },
      "source": [
        "# **Defining Training Function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qaKZr_P1CQo"
      },
      "outputs": [],
      "source": [
        "def Train(data_dir=None, CNN='ResNet50', ftr_size=64, weight_sharing=True, out_file=None):\n",
        "\n",
        "  # defining some training parameters\n",
        "  batch_size = 4\n",
        "  learning_rate = 0.00006\n",
        "  num_classes = 3\n",
        "  size = 224\n",
        "\n",
        "  # If the log file is present apppend to it\n",
        "  if out_file in os.listdir(\"/content/drive/MyDrive\"):\n",
        "    print(\"output file already exists\")\n",
        "    log_file = open(\"/content/drive/MyDrive/\"+out_file, 'a')\n",
        "  else:\n",
        "    # If not, create it\n",
        "    print(\"created log file\")\n",
        "    log_file = open(\"/content/drive/MyDrive/\"+out_file, 'w')\n",
        "    log_file.write(\"FORMAT: CNN multi_stage ftr_size weight_sharing augmented Acc M_acc N_acc K_acc M_sen N_sen K_sen M_spec N_spec K_spec M_per N_per K_per loss\")\n",
        "    log_file.write(\"\\n\\n\\n\")\n",
        "\n",
        "  # Use GPU if available, use CPU otherwise\n",
        "  if torch.cuda.is_available():\n",
        "    print(\"cuda is available\")\n",
        "    device = torch.device(\"cuda\")\n",
        "  else:\n",
        "    print(\"cuda is not available\")\n",
        "    device = 'cpu'\n",
        "\n",
        "  # Define the transform to applied to each image\n",
        "  transform = transforms.Compose(\n",
        "\n",
        "    [\n",
        "\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Resize((size, size), antialias=None)\n",
        "\n",
        "    ])\n",
        "\n",
        "  print(\"device:\",device)\n",
        "\n",
        "  # define the train, validation, and test datasets\n",
        "\n",
        "  train_dataset = datasets.ImageFolder(root=data_dir+\"/train\", transform=transform)\n",
        "  train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "  val_dataset = datasets.ImageFolder(root=data_dir+\"/valid\", transform=transform)\n",
        "  val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "  test_dataset = datasets.ImageFolder(root=data_dir+\"/test\", transform=transform)\n",
        "  test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "  size = 224\n",
        "  n_epochs = 3\n",
        "\n",
        "  # build model\n",
        "  model = build_model(CNN, weight_sharing, ftr_size, num_classes)\n",
        "\n",
        "  val_loss = []\n",
        "  val_acc = []\n",
        "  train_loss = []\n",
        "  train_acc = []\n",
        "  test_loss = []\n",
        "  test_acc = []\n",
        "  best_val_acc = 0\n",
        "  best_train = 0\n",
        "  best_test_acc = 0\n",
        "  best_test_loss = 2 ** 30\n",
        "  best_conf_mat = [[0 for i in range(3)] for j in range(3)]\n",
        "\n",
        "  # Move model to device (GPU or CPU)\n",
        "  model.to(device)\n",
        "\n",
        "  # Define loss function and optimizer\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  total_step = len(train_dataloader)\n",
        "  total_val = len(val_dataloader)\n",
        "  last_best = 0\n",
        "  finished = False\n",
        "  out_dir = 'Models'\n",
        "  name = None\n",
        "\n",
        "  # Train for the specified number of epochs\n",
        "  for epoch_num in range(n_epochs+1):\n",
        "    print(\"Epoch:\",epoch_num)\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    print(\"Training...\")\n",
        "    # Go through training dataset and update weights after each batch\n",
        "    for data, target in tqdm(train_dataloader):\n",
        "      data, target = data.to(device), target.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      outputs, img_with_rect = model(data)\n",
        "      loss = criterion(outputs, target)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      running_loss += loss.item()\n",
        "      _,pred = torch.max(outputs, dim=1)\n",
        "\n",
        "      correct += torch.sum(pred==target).item()\n",
        "\n",
        "    train_acc.append(100*correct / len(train_dataset))\n",
        "    print(\"Train acc:\",train_acc[-1])\n",
        "    train_loss.append(running_loss/total_step)\n",
        "    print(\"train loss:\",train_loss[-1])\n",
        "\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      model.eval()\n",
        "      val_conf_mat = [[0 for i in range(3)] for j in range(3)]\n",
        "      print(\"validating...\")\n",
        "      # Go through validation dataset\n",
        "      for data, target in tqdm(val_dataloader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        outputs, img_with_rect = model(data)\n",
        "        loss = criterion(outputs, target)\n",
        "        running_loss += loss.item()\n",
        "        _,pred = torch.max(outputs, dim=1)\n",
        "        for i in range(target.shape[0]):\n",
        "          val_conf_mat[int(target[i].item())][int(pred[i].item())] += 1\n",
        "\n",
        "        correct += torch.sum(pred==target).item()\n",
        "\n",
        "      val_loss.append(running_loss/total_val)\n",
        "      val_acc.append(100*correct / len(val_dataset))\n",
        "      # If the model achieves the best validation accuracy,\n",
        "      # run the model through the test set\n",
        "      if best_val_acc < val_acc[-1]:\n",
        "        best_val_acc = val_acc[-1]\n",
        "        print(\"val acc:\",best_val_acc)\n",
        "        test_conf_mat = [[0 for i in range(3)] for j in range(3)]\n",
        "        correct = 0\n",
        "        running_loss = 0\n",
        "        print(\"testing...\")\n",
        "\n",
        "        for data, target in tqdm(test_dataloader):\n",
        "          data, target = data.to(device), target.to(device)\n",
        "          outputs, img_with_rect = model(data)\n",
        "          _,pred = torch.max(outputs, dim=1)\n",
        "          loss = criterion(outputs, target)\n",
        "          running_loss += loss.item()\n",
        "\n",
        "          for i in range(target.shape[0]):\n",
        "            test_conf_mat[int(target[i].item())][int(pred[i].item())] += 1\n",
        "\n",
        "          correct += torch.sum(pred==target).item()\n",
        "\n",
        "        acc = round(100*correct / len(test_dataset),2)\n",
        "        loss = round(running_loss / len(test_dataloader), 4)\n",
        "        test_acc.append(acc)\n",
        "        test_loss.append(loss)\n",
        "        if acc > best_test_acc:\n",
        "          best_test_acc = acc\n",
        "          best_conf_mat = test_conf_mat\n",
        "          best_test_loss = loss\n",
        "          print(\"test acc:\",best_test_acc)\n",
        "          print(\"test loss:\",best_test_loss)\n",
        "          print(\"conf_mat:\",best_conf_mat)\n",
        "          torch.save(model.state_dict(), \"/content/drive/MyDrive/melanoma_detection_epoch\"+str(epoch_num)+\".pt\")\n",
        "\n",
        "      model.train()\n",
        "\n",
        "  # Compute evaluation metrics and log results\n",
        "  acc, sensitivity, specificity, percision = eval_metrics(best_conf_mat)\n",
        "  log_results(CNN, ftr_size, weight_sharing, log_file, best_conf_mat, best_test_acc, acc, sensitivity, specificity, percision,\n",
        "                                                                                                                      best_test_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train and Return a Binary Model**"
      ],
      "metadata": {
        "id": "OaJM7l06k2ym"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyrd7CWMlmQJ"
      },
      "outputs": [],
      "source": [
        "data_dir = \"/content/drive/MyDrive/ISIC_2019_subset\"\n",
        "\n",
        "Train(data_dir = data_dir, CNN='ResNet50', ftr_size=64, weight_sharing=True, out_file='results.txt')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}